{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1TI12MbimvdsLdih9NPRo0li_geUr-EHl",
      "authorship_tag": "ABX9TyMvfE3OQ4tRcajhnNWyOF5o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarshanaHeendeniya/ML_Learning/blob/main/Healthcare_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rlSHcSIjJu0F"
      },
      "outputs": [],
      "source": [
        "# Importing required libraries\n",
        "import random\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "intents = json.loads(open(\"intent.json\").read())\n",
        "\n",
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "\n",
        "ignore_letters = [\"?\", \"!\", \".\", \",\"]\n",
        "\n",
        "for intent in intents[\"intents\"]:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        word_list = nltk.word_tokenize(pattern)\n",
        "        words.extend(word_list)\n",
        "        documents.append((word_list, intent[\"tag\"]))\n",
        "\n",
        "        if intent[\"tag\"] not in classes:\n",
        "            classes.append(intent[\"tag\"])\n",
        "words = [lemmatizer.lemmatize(word)\n",
        "         for word in words if word not in ignore_letters]\n",
        "\n",
        "words = sorted(set(words))\n",
        "classes = sorted(set(classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6LSLCJgBnnd",
        "outputId": "a6bb2716-0c72-47d7-b252-ecd036e51607"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(words, open('words.pkl', 'wb'))\n",
        "pickle.dump(classes, open('classes.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "hRsCwoSnKALp"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}